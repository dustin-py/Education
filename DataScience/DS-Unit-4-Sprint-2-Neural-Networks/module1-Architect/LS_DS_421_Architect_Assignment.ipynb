{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_432_TensorFlow_Assignment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObyHCH8HvHSf",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# *Data Science Unit 4 Sprint 2 Assignment 1*\n",
        "\n",
        "Use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
        "\n",
        "*Don't forgot to switch to GPU on Colab!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Tc3ovEyQ9b",
        "colab_type": "text"
      },
      "source": [
        "## Load Your Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkU0pAYCvU8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "\n",
        "data = np.load('quickdraw10.npz')\n",
        "X = data['arr_0']\n",
        "y = data['arr_1']\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qsDqdqvHDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['apple',\n",
        " 'anvil',\n",
        " 'airplane',\n",
        " 'banana',\n",
        " 'The Eiffel Tower',\n",
        " 'The Mona Lisa',\n",
        " 'The Great Wall of China',\n",
        " 'alarm clock',\n",
        " 'ant',\n",
        " 'asparagus']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owbm1EbxvA5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "start = 0\n",
        "\n",
        "for num, name in enumerate(class_names):\n",
        "    plt.subplot(2,5, num+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X[start].reshape(28,28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(name)\n",
        "    start += 10000\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c97_M1WNvTNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need this line to randomly shuffle both the X & y at the same time.\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "X, y = shuffle(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb70CbLVyK65",
        "colab_type": "text"
      },
      "source": [
        "## Build Your Baseline Model\n",
        "Some Hints:\n",
        "\n",
        "\n",
        "*  Model should have 784 input values (like mnist)\n",
        "*  Use `sparse_categorical_crossentropy` as your loss function.\n",
        "* You need 10 neurons in your last layer for output\n",
        "* You can add as many hidden layers with as many neurons in them as you like. \n",
        "* Limit your model epochs to 30 each time you fit.\n",
        "* You can use the `validation_split` command to automatically create a training / validation dataset.  Specify a percentage such as .2 in your fit statement. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHWblzsMyNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0QJURWh-9uv",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the results\n",
        "\n",
        "Create charts for both loss and accuracy by epoch. Use line graphs for both charts. Analyze the results. \n",
        "\n",
        "At what point should we have stopped training the model and why? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KONJtU5wqlXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijAlzfYKAFaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_records(baseline.history)\n",
        "df['epoch'] = [i for i in range(df.shape[0])]\n",
        "\n",
        "ax = sns.lineplot(x='epoch', y='val_loss', data=df)\n",
        "ax = sns.lineplot(x='epoch', y='loss', data=df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAhBrcE4yOZe",
        "colab_type": "text"
      },
      "source": [
        "## Change Optimizers\n",
        "Try using the keras `adam` optimizer instead of `sgd` in your model. Visualize the difference in validation loss between the models with different optimizers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIW_spOZ0cxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf.keras.optimizers import Adam\n",
        "\n",
        "adam = Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJrbh3qryi4w",
        "colab_type": "text"
      },
      "source": [
        "### Additional Written Tasks:\n",
        "In this section, you will need to search for resources: \n",
        "1. Investigate the various [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses). Which is best suited for the task at hand (predicting 1 / 0) and why? \n",
        "2. What is the difference between a loss function and a metric? Why might we need both in Keras? \n",
        "3. Investigate the various [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). Stochastic Gradient Descent (`sgd`) is not the learning algorithm dejour anyone. Why is that? What do newer optimizers such as `adam` have to offer? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzs4fd-RynDd",
        "colab_type": "text"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Research convolutional neural networks and try including convolution layers in your network.\n",
        "- Pick two classes and make QuickDraw a binary classification problem, how does your model architecture change?\n",
        "- Implement Cross Validation model evaluation on your Quickdraw implementation \n",
        "\n",
        "Watch some more videos on Gradient Descent:\n",
        "- [Gradient Descent, Step-by-Step](https://www.youtube.com/watch?v=sDv4f4s2SB8)  by StatQuest w/ Josh Starmer. This will help you understand the gradient descent based optimization that happens underneath the hood of neural networks. It uses a non-neural network example, which I believe is a gentler introduction. You will hear me refer to this technique as \"vanilla\" gradient descent. \n",
        "- [Stochastic Gradient Descent, Clearly Explained!!!](https://www.youtube.com/watch?v=vMh0zPT0tLI) by StatQuest w/ Josh Starmer. This builds on the techniques in the previous video.  This technique is the one that is actually implemented inside modern 'nets. \n",
        "- [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)"
      ]
    }
  ]
}